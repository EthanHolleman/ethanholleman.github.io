[{"content":"I am almost constantly running my 3D printer. Usually the only time it is not pumping plastic is when the laundry machine is running. Both the printer and laundry machine are plugged into the same outlet and for some reason having them both running at the same time can occasionally cause the printer to freeze and stop.\nIf the girlfriend wants to start a load of laundry a common question she has for me is \u0026ldquo;How long is left on this print?\u0026rdquo; since she is nice enough to not want to risk my print failing.\nSo I wanted to create a quick and easy to read indicator for how far along my printer is through a print and came up with the solution described below. It uses a Raspberry Pi pico W (a microcontroller that can connect to Wifi) to ask the computer in charge of the 3D printer how much time is left in the current print as a percentage. The Pico then displays this percentage on a strip of individually addressable LEDs called NeoPixels to convey that information to an outside observer. While this does not tell the observer the exact amount of time left in the print it does still act as a quick reference.\nBelow is a gif of the system in action.\nDetails for makers This section is how to actually build this device if you are interested. Follow the sets outlined below.\nGather materials You will need\nRaspberry Pi Pico Some kind of NeoPixel strip (I used this one from Adafruit) Jumper wires Solder and soldering iron 3D printer and running OctoPrint server Get soldering First solder jumper wires to the three pads on your NeoPixel strip. Be careful to note which wire is soldered where. You of course do not need to use jumper wires like I did. This is not the cleanest method but it is easy and does not require stripping wires.\nThen connect these wires to your Pico in the following way, you can use the RPi Pico pinout for referencing pin numbers.\nNeoPixel GND -\u0026gt; Rpi Pico Pin 38 NeoPixel 5VIN -\u0026gt; Rpi Pico Pin 36 (3V3 OUT) NeoPixel DIN -\u0026gt; Rpi Pico Pin 34 (GPIO 28) Get programming Configure and IDE and programming environment to write MicroPython code to your Pico. Setting this up is beyond the scope of this guide and has been well covered by other sources, I recommend using Thonny and following the official Raspberry Pi Foundation getting started guide.\nOnce you are able to connect to your Pico (it is good to test everything is working by running the \u0026ldquo;blink\u0026rdquo; program in the getting started guide) you can download the project code from this GitHub repo (code is located in the src folder) and transfer all files within the src folder to your Pico.\nModify secrets.py Your Pico needs to do two things in order to know the status of your 3D printer; connect to the same Wifi network your OctoPrint server is running on, and access the OctoPrint API.\nBoth of these actions require credentials which are stored in secrets.py file. There are three values that need to be populated in this file\nSSID: The name of your wifi network PASS: The password to this wifi network API_KEY: An OctoPrint API key If you are unsure of how to generate an OctoPrint API key please see this section of the OctoPrint documentation.\nFlash code Check everything is working by running main.py in Thonny. Your Pico should flash it\u0026rsquo;s internal LED three times and then display your printer\u0026rsquo;s status using the NeoPixel strip.\nPosition pixels and Pico Assuming everything is working all there is to do now is pick a suitable place for your Pico and NeoPixels. I mounted the NeoPixels directly to the front of my print cabinet using 2 m2.5 screws.\nI then printed this case by GrevTech and used it to mount the Pico to the side of the cabinet also with m2.5 screws and ran the excess jumper wires under the case to keep them slightly more controlled.\nWhile this is not the cleanest setup of all time I was able to complete this entire build in about 3 hours and am pretty happy with it. In the future I may increase the symbology that the Pico can communicate in order to display some additional information like nozzle temperature.\n","date":"2022-11-27","permalink":"https://ethanholleman.com/posts/pico-print-status-indicator/","tags":["blogs","3D printing","Octoprint"],"title":"Raspberry Pi Pico W LED 3D print status monitor"},{"content":"This is a project I put together for the parents before I headed home for the weekend using the Glowforge laser cutter at the maker space at the UC Davis craft center.\nThe project consisted of generating street maps of a few personally notable places using SnazzyMaps. I converted these maps to bitmaps using Inkscape which I also used for the rest of the coaster design. Coasters were cut out of medium thickness clear acrylic sheets.\nTo make the box, I used MakerCase to generate the SVG plans based on the size of the coasters. I cut the box sections from medium thickness draft board and assembled with tacky glue.\nOverall I was really happy with the resolution of engraving that the Glowforge was capable of producing. One thing I noted was that the reproducibility of movement between runs was poor. This means if you have a run with a cut that does not make it all the way through your material and you run the cut again there is a decent chance the second cut will not line up exactly with the first. To avoid this I would generally do two passes on all cuts, even if I was pretty sure it was not needed just to ensure I would always get a clean through-and-through cut with one run.\n","date":"2022-10-15","permalink":"https://ethanholleman.com/posts/streetmap-coasters/","tags":["blogs","laser cutting"],"title":"Laser cut street map coasters using Glowforge"},{"content":"A little while ago I was at Bay Area Ultra Stock\u0026rsquo;s 40th meetup and managed to record a few of the games. Played at Braly Elementary school in Sunnyvale California.\nFor these games I was playing with my Pump Skewer by SillyButts designs. All parts were printed on my modified Ender3V2.\n","date":"2022-10-09","permalink":"https://ethanholleman.com/posts/baus-40/","tags":["blogs","Nerf","BAUS"],"title":"BAUS 40 Sunnyvale: Gameplay footage"},{"content":"The site banner image is actually a specific type of infill pattern used in 3D printing called \u0026ldquo;gyroid infill\u0026rdquo;.\nGenerally, to save time and plastic, parts that are 3D printed are not printed as 100% solid blocks. Instead the user will set a infill percentage that defines how much of the interior space is filled. Once that is decided you still need to determine exactly how you are going to fill that space so it can be translated into commands the printer can understand.\nEnter infill patterns.\nImage courtsey of Cura\nThese are strategies used to fill in the internal space of a printed object. One of the most popular for high strengths prints is gyroid infill because it is equally strong in all directions and optimizes material use.\nI think the pattern it creates also looks very interesting.\nI created these images by screenshoting a layer of a print I was setting up using gyroid infill and then manipulating the image in InkScape to create a high resolution bitmap.\nThanks for your curosity!\n","date":"2022-10-02","permalink":"https://ethanholleman.com/posts/sitebanner/","tags":["about this site","blogs"],"title":"What is the site banner image of?"},{"content":"It is likely you have come into contact with the Nerf brand at some point in your life. Hasbro\u0026rsquo;s iconic line of foam flinging mechanisms, until recently, held a near complete monopoly on the hobby. While Nerf blasters remain fun for casual around-the-house or workplace battles, for many, they simply do not pack enough of a punch. There has always been a small continent of nerds who longed to maintain the goofy persona that Nerf affords with its bright colors, oversized projectiles, and exaggerated designs but push the limits of their blasters. These early \u0026ldquo;hobby grade\u0026rdquo; Nerfers modified existing blasters and even constructed completely original designs using parts you could buy from a hardware store. Today, this \u0026ldquo;hobby grade\u0026rdquo; side of Nerf has evolved into a rich and diverse ecosystem complete with an active community, terminology and small businesses. This article attempts to give a brief overview of this side of the hobby, what it involves, and how to begin participating.\nA breif aside on terminology Nerf, like Frisbee, is so ubiquitous that it is both the brand and the verb for foam dart tag which is itself any activity / game that involves launching foam darts at others in order to \u0026ldquo;tag\u0026rdquo; them. Here they are used interchangeably. Some other terms to be aware of are listed below.\nStock: Dart tag games that utilize unmodified Nerf brand blasters. Superstock: Dart tag games that allow some degree of modification largely to Nerf brand blasters. This might include removing performance limiting mechanisms or installing new components in store bought blasters. Ultrastock: Dart tag in this category often involves completely custom built blasters. Eye protection is mandatory and games begin to move out of the backyard Nerf war feel to more paintball like. The practical, on the ground dividing line for these classifications tends to be the velocity at which blasters can propel darts, usually measured in feet per second (FPS). Stock games range from 50-90 FPS, superstock 90-150, and ultrastock 150 and beyond usually capping out around 250 FPS. Hobby grade is generally used interchangeably with ultrastock.\nThe advent of accessible 3D printing Within the last 4 years the super and ultra stock Nerf community have experienced what is likely a literal exponential growth. This has been largely precipitated by the advent of affordable, accessible and reasonably reliable hobby grade 3D printer kits. This in combination with the availability of free professional grade 3D modeling software has resulted in an explosion in what can only really be termed \u0026ldquo;Nerf technology\u0026rdquo;.\nMy modified Ender 3V2 printer The 3D printer I use for all my Nerf related prints. I have added parts (some printed on this machine) in an attempt to increase performance and reliability. The Ender3 is a common workhorse machine due to its low cost at the price of requiring occasional tinkering in order to keep running consistently.\nThese conditions have allowed talented designers and engineers to experiment with, and rapidly prototype new, entirely 3D printed high performance blasters. Unlike many hobbies where innovation is driven in a top down manor, with companies producing and marketing new and more performant designs, mechanisms and tools, the distributed nature of 3D printing and (until recently i.e DartZone) the lack of willingness from large toy companies to produce high performance blasters has meant that innovation in hobby grade Nerf has been driven and proliferated almost entirely from the bottom up. Today the highest performing blasters have been designed entirely by members of the community.\nPopular open source blasters Top to bottom, right to left. The Caliburn a spring powered blaster by CaptainSlug. The Lynx a bullpup springer by Orion Blasters that has gained popularity in the competitive nerf community. The Gryphon, a flywheel powered blaster by Flygonial. Talon Claw, a shorter more compact version of the Caliburn also by CaptainSlug. The files for all of these blasters are freely available for anyone to download and print. Parts that cannot be printed (springs, screws, metal rails) are often sold as hardware kits by the designer.\nThis has fostered an open-source spirit, with most designers making the files for their blasters freely available so anyone with a 3D printer can utilize them. The impact 3D printing has had on hobby grade dart tag cannot be overstated. In many ways the hobby largely owes its very existence to the fact almost anyone can operate a 3D printer in their home (mine currently runs in the laundry room).\nThe ability to create almost any shape out of relatively high strength plastics at home has allowed the hobby to move beyond modifying store bought blasters with upgraded components to designing their own foam flinging systems from the ground up. Designs that work well spread from printer to printer and rapidly inform the \u0026ldquo;most effective tactics available\u0026rdquo; or Meta of the hobby for competitive players. This has created a community of \u0026ldquo;distributed prototyping\u0026rdquo; where designers semi-independently experiment with new configurations; this in aggregate appears as a spiderweb of optimizations for a given system.\nDue to this extreme utilization of 3D printing has meant that participation at the highest levels of hobby grade nerf (ultra stock) usually requires the ability to understand and operate a 3D printer, some degree of CAD skills, and the mechanical ability to assemble blasters with minimal instructions. This is likely the largest barrier to entry into the competitive side of the hobby for many. Completely assembled 3D printed blasters can be purchased from a variety of independent online shops and boutique sellers but due to the low throughput of 3D printing these pre-assembled blasters are better compared to commissioning custom handmade cabinetry for your kitchen then purchasing an Ikea table.\nDue to this fact assembled competitive blasters often range in the $200-400 price range. This is sometimes even more expensive then the machine the parts were actually printed on. This will hopefully be remedied to some degree in the future as more reliable affordable 3D printers make their way onto the market.\nEmergence of competitive dart tag competitive The significantly improved performance of ultrastock blasters has allowed a completely new style of dart tag games to emerge that bears more resemblance to paintball than traditional Nerf wars.\nFoam Pro Tour 2022 logo. Courtesy of foamprotour.com.\nOne of the largest and most popular tournaments being the Foam Pro Tour in which dart tag teams from around the country compete for what is effectively the Super Bowl of Nerf.\nImages from a practice match with team BAD; my competitive dart tag team. Images courtesy of fish855 via flickr.\nCompetitive dart tag matches are typically extremely fast paced, lasting less than 3 minutes per round and therefore place high emphasis on agility, speed, accuracy and team strategy. Competitive Nerf matches, while still relatively rate, tend to be open to anyone who would like to participate and is willing to sign a waiver.\nFinding a dart tag group near you For those who see the challenge of building and tuning their blasters and gear as part of the fun (or for whom cash is no object) a number of super and ultrastock communities have formed across the US and the world.\nLogo for the Bay Area Ultra Stock (BAUS) dart tag group with whom I tend to play most of my ultrastock games with. If you are in the San Fransico Bay Area you can get involved by joining the group\u0026rsquo;s discord at this link.\nThe r/Nerf subbreddit has even assembled a Google Map listing Nerf groups to make it easier to find a community in your area. Nerf groups tend to consistent of nerdy, welcoming and friendly individuals. Depending on the types of games played (higher stock groups tend to skew a bit older) you will find people of all ages.\nLooking to the future Hobby grade dart tag has just graduated high school and is beginning its early adult life. It is a great time for anyone interested to get involved. Groups are numerous enough as to if you live near a major population center there is likely to be a sizable organization to get involved with. There are seemingly new freely available blasters and designs being released every week by members of the community so for the 3D printing nerfer there is always a new build to tackle.\nWhile this article focused on the competitive and ultrastock levels of dart tag as that is what I am most personally involved in there is enough diversity of interest that anyone can be involved at any level with no requirement for technical skills. A great place to start for information is the r/Nerf subbreddit side bar which has info on hosting your own Nerf War, links to discord servers, and an extensive hobby blaster guide.\nWhere ever your interest may lie, we are undoubtedly living in a golden age of the dart tag hobby; now go and bask in its glory.\n","date":"2022-06-12","permalink":"https://ethanholleman.com/posts/hobby-grade-nerf/","tags":["3D Printing","blogs","Nerf","Foam Pro Tour"],"title":"Rejoice, the golden age of Nerf is upon us! A general overview of the state of hobby grade and competitive foam dart tag"},{"content":"These recommendations are in addition to general 3D printing best practices, namely proper bed leveling and cleaning. Without a clean and level bed, none of this advice will do you any good.\n1. Print a filament guide; preferably one that uses a bearing for your printer I print on an Ender3 and use this guide by Mark Villela. A filament guide is an abosulte requirement to reduce the angle the filament must enter the extruder gear at. Without one, the tension on the filament at the extruder will be to great for it to push filament into the hot end.\n2. Adjust slicer settings The default settings for generic TPU 95A in Cura were not conservative enough to produce consistent prints. You need to print slow, hot and without retractions.\nYou can download my Cura TPU profile at this link. If Cura is not your preferred slicer a summary of the settings I changed are shown below.\nadhesion_type = skirt material_bed_temperature_layer_0 = 70 alternate_extra_perimeter = True material_print_temperature_layer_0 = 228.0 retraction_enable = False speed_layer_0 = 10 speed_print = 20.0 3. Clean your nozzle The stringy nature of TPU can cause it to stick to your printer\u0026rsquo;s nozzle. If there is a significant amount of TPU present on the nozzle before the print begins, this can cause a spiral of extruded filament buildup on the nozzle instead of the print bed. Clean the nozzle before starting or if its too far gone just save yourself some time and replace it with a cheap brass 0.4 mm.\n4. Print models one at a time I have found that when you disable retractions you can get a lot of stringing between objects. This is greatly reduced when you keep your prints to one main object or print objects serially instead of all-at-once.\n","date":"2022-03-08","permalink":"https://ethanholleman.com/posts/print_tpu/","tags":["3D Printing","blogs"],"title":"Print TPU without a direct drive extruder"},{"content":"Update for Jammy Jellyfish users: May 2022 If you recently updated to Ubuntu 22.04 LTS (Jammy Jellyfish) you may have found yourself unable to connect to Eduroam. This seems to be due to a weird ssl issue with Eduroam that I do not currently pretend to understand but no one cares about that anyway. You want answers. Here is what worked for me which is based off of this bug report.\nOpen /usr/lib/ssl/openssl.cnf in your favorite text editor. Add the following lines to the beginning of the file. openssl_conf = openssl_init [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] Options = UnsafeLegacyRennegotiation Save and reboot. Attempt re-connecting with the according to this guide but use your Davis CAS credentials. Good luck.\nPre-2022: Leaving up for archival purposes I use Ubuntu 20.04 as my daily driver operating system at work and at home. Until recently, when I was on campus I had a really hard time consistently connecting to the Eduroam network on my Linux machine; usually resorting to guest wifi (disgusting) and requesting a new login every two weeks after my credentials expired.\nA few days ago I decided I was going guest wifi cold turkey and returned to by search for any resources that might be available. The Davis IT knowledge hub suggests you run a Eduroam provided Python script configuration program. This worked on one of my machines and failed on another. Additionally, it may require storing your password in plain text (don\u0026rsquo;t do this). I had much better luck following this guide from the physics department. However it is outdated and there is one key change you must make for the instructions in this guide to work.\nThe guide instructs you to use a file located at /etc/ssl/certs/AddTrust_External_Root.crt as your CA certificate when configuring the connection. Unless you have an older machine this file will probably not exist. This is because this particular certificate expired in May 2020 and has been updated. Instead, make sure to use the certificate located at /etc/ssl/certs/USERTrust_RSA_Certification_Authority.pem. This path may be slightly different if you are using a different distro. Follow all other instructions provided by the physics department.\nHope this is helpful!\n","date":"2021-08-19","permalink":"https://ethanholleman.com/posts/davis-wifi-on-linux/","tags":["blogs","Davis guides","Linux"],"title":"How to actually connect to UC Davis campus wifi (Eduroam) on Linux"},{"content":"I have been interested in education since my undergraduate career. While working towards my undergrad degree in bioinformatics, I designed and presented a short series of lectures at a local assisted living facility focused on building proficiency with technology as a means of independence. The series covered the very basics of computer and internet usage; how to email a picture, write a letter with a word processor, and spot common signs of phishing scams. The hour sessions were, in all honesty, slow, painful, and inexplicably seemed to elicit pseudo-existential questions from the participants like \u0026ldquo;Where does a window end and another begin?\u0026rdquo; but ultimately were gratifying. While this by no means constituted a full-fledged-credited-course, I got a great crash course and a reality check in what is involved in creating a small curriculum and how to stay flexible when it all falls apart in the middle.\nThis type of experience got me more interested in how I could get involved in personal mentorship entering grad school, and over the past year I participated in the Sheldon High School Biotechnology mentoring program, where I answered questions from and just generally got to know a student from Sheldon High in the program interested in bioinformatics and computer science. I now hope to take on similar, more official responsibilities and contribute to our graduate programs educational curriculum as the educational policy officer.\nAs the educational policy officer, I would first like to build on the excellent foundation Shannon and others have laid through the 2021 GGG298 course. I would want to work with her as she transitions out of the position to review both what she and previous students enjoyed / thought could be improved about the course and poll the incoming cohort to learn what new areas of focus might be of interest to incoming students.\nA significant role of the education policy officer is to access internal fellowship applications from current and incoming IGG students. In this effort I would both work to make sure fellowship deadlines are well advertised throughout the IGG community and ensure any evaluations reflect criteria established by the IGG admissions committee and, if administratively possible possible are publicly available to applicants.\nFinally, as the educational policy officer, I would focus on the accessibility of IGG courses as we transition back to an in-person model. I was struck recently by this New York Times Opinion piece focusing on the challenges faced by many in \u0026ldquo;returning to normal.\u0026rdquo; An essential sentiment being along the lines of \u0026ldquo;Why were we able to make the world virtually accessible only when it mattered for fully-abled people.\u0026rdquo; Remote learning offers considerable advantages to many people. We would be remiss not to take this opportunity to improve what is accepted as normal using our experiences from the past year. With input from other IGG students and facility I hope to identify routes to increase the accessibility of our coursework.\nThank you for your consideration, and please feel free to reach out to me with any questions.\n","date":"2021-07-19","permalink":"https://ethanholleman.com/posts/epo/","tags":["blogs","IGG"],"title":"2021 candidacy for IGG education policy officer"},{"content":"Over the past academic year, I have worked as a member of the Legislative Affairs Committee (LAC); a GSA committee that works to track federal and state policy that may impact UC students as it is proposed and progresses through the legislature. The reviews that the committee performs eventually help inform the positions of the UC Graduate and Professional Council who\u0026rsquo;s members advocate for graduate and professional students to the UC Regents and in state and federal legislatures.\nMy focus as part of the committee was to improve the efficiency of the review and tracking of State legislative items. After learning how legislative tracking is done within the committee I identified monotonous tasks like navigating the California legislative website, searching for keywords, and summarizing bills and built a program to automate these tasks, which is freely available for anyone to use.\nThrough my experience with the LAC, I became more familiar with GSA processes and its membership, and I am now looking for a way to represent more directly the interests of our graduate group using what I have learned. While the commitment to the position is only monthly; I hope to look for creative and alternative ways to improve aspects of the position as I did in the LAC, and provide active, high-quality representation of our graduate group at the GSA.\n","date":"2021-07-19","permalink":"https://ethanholleman.com/posts/gsa_rep/","tags":["blogs","IGG"],"title":"2021 candidacy for IGG GSA representative"},{"content":"I took the day off today to recover from second round of the covid vaccine and spent a little while implementing my own version of Conway\u0026rsquo;s Game of Life in Python.\nHere\u0026rsquo;s an example of a simulation involving a grid of 500 x 500 cells. When initializing the first generation each cell had a 0.3 probability of starting as a living cell (start-as-living probability). Increasing this value will increase the number of cells the grid is initialized with. I ran the simulation for 100 generations.\nand with a 0.1 start-as-living probability.\nI was curious how changing the starting probability of a cell starting the simulation as living would effect the game as it progressed. So I ran 250 generation simulations with start-as-living-cell probabilities of 0.1, 0.3, 0.5 and 0.8. Each start-as-living-cell probability plot is composed of 100 simulations. The generation number is shown on the x-axis and the proportion of living cells is shown on the y.\nThe results where pretty much as one might expect given the game\u0026rsquo;s rules. Intermediate starting populations level off as the number of generations increase. What was interesting to see was how quickly the number of cells drop if the population exceeds the \u0026ldquo;carrying capacity\u0026rdquo; of the grid.\nIf you are interested you can download my implementation from GitHub at this link.\n","date":"2021-04-16","permalink":"https://ethanholleman.com/posts/conway_game_of_life_implementation/","tags":["programming","Python","blogs"],"title":"Implementing and visualizing Conway's Game of Life"},{"content":"Quick stop at Truckee lake on the way out of California.\nTwo abandoned structures just outside of Hawthorn, Nevada.\nThe somewhat ill-conceived Clown Motel. Although as someone who as worked with professional clowns I am not one to malign the occupation; clowns are an extremely hard working bunch.\nMost roads past Reno heading towards Death Valley look something like this.\nInvasive Burros (donkeys) running through the scrub. Burros were unintentionally introduced to Nevada after escaping from early mining towns.\nLady Desert The Venus, part of the Goldwell Open Air Museum by Dr.Hugo Heyrman in Rhyolite, Nevada; an abandoned mining town. Constructed before the popularization of Minecraft.\nThe Last Supper (1984) by Charles Albert Szukalski also at Goldwell.\nJust outside Artist\u0026rsquo;s Palette overlooking the valley.\nArtist\u0026rsquo;s Palette.\nErica in the Artist\u0026rsquo;s palette canyons.\nView from Zabriskie Point.\nSalt Creek, home of one of ~ 10 species of California Pupfish, a really classic and interesting example of allopatric speciation. The 10 species diverged from a single ancestor when the drying of ancient lake Manley separated the populations into isolated pockets.\nPupfish closeup.\nErica walking across the Mesquite flats sand dunes. Picnic in the Mesquite dunes around dusk.\nSunset at Mesquite dunes.\n","date":"2021-03-25","permalink":"https://ethanholleman.com/posts/death_valley/","tags":["","national parks","blogs"],"title":"Trip to Death Valley National Park"},{"content":"A while ago when I was reading through my graduate group\u0026rsquo;s weekly bullitin I saw a solicitation for the Cornell Institute For Digital Agriculture (CIDA) Hackathon.\nI had not participated in a Hackathon before and it looked interesting so I applied, was selected, and kind of forgot about it until last weekend when I needed to find and join a team. I ended up finding a great group of students from the Netherlands, Brazil and Ohio worked with them on our project proposal basically my entire waking Saturday.\nOur group decided to focus on the food waste challenge category with out central idea being an app that would connect food waste medium scale food waste producers like restaurants to similar scaled consumers, such as farmers who utilize food wastes for composts or animal feed. Producers would offer a starting bid for a pickup and consumers could make counter offers. In theory, everyone ends up benefiting because producers can offer below landfill tipping rates and farmers get paid to utilize a resource they normally would in the best case get for free.\nOriginally, I was expecting to be focused on programming the entire time but there was much more emphasis on business-ish variables like markets, user-base and that type of thing which I don\u0026rsquo;t usually spend much time thinking about.\nOverall, it was a lot of fun and we actually ended up winning one of the five finalist categories, most innovative, out of around a total of 30 teams from around the world and taking home $1500 for our team. I really enjoyed the \u0026ldquo;do this thing in an unrealistically short amount of time but there isn\u0026rsquo;t any toxic pressure\u0026rdquo; environment and it really facilitated connection between everyone in the group. I\u0026rsquo;m still in contact with everyone I worked with (seen in our team picture below from the hackathon website).\n","date":"2021-03-09","permalink":"https://ethanholleman.com/posts/cida_hackathon/","tags":["hackathon","blogs"],"title":"Finalist at the Cornell Institute For Digital Agriculture Hackathon!"},{"content":"Recently I was working on automating some legislation tracking tasks for the UC Davis Legislative Affairs Committee and noticed a potential bug in displaying the results of bill information searches and wanted to note it.\nIf you are interested in California legislation you can go to the state website and search for bills by a number of parameters, including by keywords.\nWhich brings up a number of bills. Clicking on one gives us the bill\u0026rsquo;s full text with out keyword highlighted.\nThe bug occurs when we search for specific html tags like div or span.\nWhen these terms are searched the source html ends up being rendered on the actual page with tags highlighted.\nLooking at the actual html at locations where this occurs it looks like the highlighting is resulting in unescaped \u0026lt; characters.\n\u0026lt;div style=\u0026quot;margin:0 0 1em 0\u0026quot;\u0026gt;\u0026lt;\u0026lt;b\u0026gt;\u0026lt;span style='background-color:yellow'\u0026gt;span\u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt; \u0026lt;/\u0026lt;b\u0026gt;\u0026lt;span style='background-color:yellow'\u0026gt;span\u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt;\u0026gt;\u0026lt;\u0026lt;b\u0026gt;\u0026lt;span style='background-color:yellow'\u0026gt;span\u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt; This bug seems to basically be an aesthetics issues and may confuse a user or two who\u0026rsquo;s search terms overlap with html tags but that is about it. I opened an issue on the state website to inform the web administrator.\n","date":"2021-02-24","permalink":"https://ethanholleman.com/posts/california_legislature_search/","tags":["blogs","programming","bugs"],"title":"Bug in California legislative information bill search text highlighting"},{"content":"Polo: an open-source graphical user interface for crystallization screening was just published in the Journal of Applied Crystallography. It is my first academic paper as well as my first first-author paper.\nPlease check out the article at this link!\nBelow are a few of the figures from the article, click to be taken to the full descriptions.\n","date":"2021-02-20","permalink":"https://ethanholleman.com/posts/polo_published/","tags":["programming","blogs","Python"],"title":"Polo paper published in the Journal of Applied Crystallography!"},{"content":"Recently I was asked by a friend if they knew about any databases that classified cannabis strains by symptoms people tend to use them to relieve. I didn\u0026rsquo;t know of the existence of any but had heard about leafly.com which catalogues user reviews of various cannabis strains and compiles data on their characteristics.\nI thought this could be a good place for them to start and so I started looking into what it would take to make a webscrapper to pull down all the data leafly has complied on hundreds on cannabis strains.\nIt turns out it didn\u0026rsquo;t take that much.\nFirst, you can browse strains by going to the strains url at https://www.leafly.com/strains. Conveniently, you can iterate through pages by just adding ?page=n where n is whatever page you want.\nI started looking through the html of these pages originally to find a way to pull out urls that would lead to each individual strain\u0026rsquo;s page but leafly (conveniently for me) provides basically all the information it has on the strains listed on a specific page in a nicely formatted json string in a script at the bottom of the page.\nI was using the BeautifulSoup package and so after getting the page content with requests all it took was\njson_script = str(soup.find_all('script', id='__NEXT_DATA__')) clean_json = re.sub('\u0026lt;.*?\u0026gt;', '', json_script) json.loads(clean_json).pop() to extract, clean and load the json into a dictionary. I did this for all pages and collected 167 json files.\nI then threw together a quick python script to pull out basic attributes about each strain from the json file collection including metrics on the terpene content of each strain and the feelings strains tend to create for users, collectively called \u0026ldquo;effect measurements\u0026rdquo;. These were both expressed as floats but I am not sure what the units could be for either if there are any.\nFirst I did some visualization in R to see if there were any differences between the phenotypes that immediately popped out. Below are two heatmaps comparing effect measurements and terpenes (right to left respectively). The color of the bar on the y-axis corresponds to the strains phenotype.\nWhile there was not obvious clustering when using terpene measurements, there was slightly more promising results going off the effect measurements; this was actually opposite what I was expecting.\nI also plotted the same attributes shown in the heatmaps as violin plots, separated by each phenotype which are shown below.\nI wanted to see if I could use classify the phenotype of each strain (sativa, indicia or hybrid) based on the various terpene and / or effect measurements in a way that was at least better than a random guess.\nI choose to go with a random forest model using the randomForest library in R since there are three possible classifications. The variable importance plots generated from the varImpPlot function after training the models on their respective training sets is below.\nWhich indicated sleepy as the most predictive measure of effect variable and caryophyllene or myrecene as the most predictive terpenes.\nI then ran each model on their validation sets and created a confusion matrices.\nTerpene confusion matrix\npredicted observed Hybrid Indica Sativa Hybrid 685 25 10 Indica 243 8 0 Sativa 123 7 3 Using terpenes as the explanatory variables was worse then guessing and basically the model just thought everything was a hybrid. This could have been because most of the strains were hybrids.\nEffect measurement confusion matrix\npredicted observed Hybrid Indica Sativa Hybrid 845 56 43 Indica 180 213 1 Sativa 157 1 91 Not great either, but better. This could suggest that user ratings are slightly more predictive of strain phenotype then leafly terpene measurements. Although, there is a lot of optimization that could still be done here. Either way, fun little exercise that traversed web-scrapping, Python and R.\nIf you would like to play around with the processed data (csv) you can download it at this link.\n","date":"2021-02-16","permalink":"https://ethanholleman.com/posts/leafly_data/","tags":["programming","blogs","Python","R"],"title":"Scraping leafly.com cannabis strain data"},{"content":"The way academic citations are measured currently is pretty standardized. Authors of article A accrue a citation whenever their article is directly cited in article B. But there is likely a large amount of work that was cited by article A but not by article B. The authors of this work which indirectly contributed to article B by contributing to article A (which B cites) will not see a citation.\nWhat if instead citing one article triggered a recursive call all the way down the network formed by articles and their citations? Would this end up eventually citing almost all articles in a field? This is basically the six degrees of separation question but the academic articles.\nI was wondering about this and so experimented on a small scale using the PMC API and a little bit of Python. Conveniently, PMC articles are indented by a unique ID, and you can use the PMC API to get the IDs of all the articles a specific article cites. With this basic functionality we can build up citation networks that branch out an arbitrary distance from one specific \u0026ldquo;root article\u0026rdquo;. This can give an idea of an articles connectedness to other academic literature and what it would look like if citing one article triggered this recursive citation cascade.\nPractically, this is limited because not all articles an article cites will be in PMC and therefore the network will be incomplete, but it is good enough for a fun experiment.\nI tested this out on this article and created the graph below by only traversing three \u0026ldquo;layers\u0026rdquo; deep into the network.\nThat would be a lot of extra citations!\nIf you would like to try this on your favorite PMC article you can download the code from the GitHub page at this link.\n","date":"2021-02-13","permalink":"https://ethanholleman.com/posts/recursive_citations/","tags":["R","blogs","Python"],"title":"What would recursive academic citations look like?"},{"content":"NBI Background The National Bridge Inventory (NBI) is a program of the Federal Highway Administration which is an agency within the U.S Department of Transportation. The NBI makes available records and statistics about all the bridges in the United States which includes information about bridge location, integrity, inspection history and usage.\nPotential encoding discrepancy As a side project I have been working on creating a more exhaustive Python package for parsing NBI data. This is mainly focused on decoding the numerical representations present in data files to their semantic meanings specified in the NBI documentation.\nI ran into errors when trying to decode the state code fields, which based on the available documentation uses the coding table below.\nThe documentation states the state code will contain three digits, but a quick check of the csv file I was using revealed most state codes had at most 2 digits.\nI wanted to see if this might be true in more files hosted by NBI and so I quickly threw together a Python script to test if this two digit encoding was present in other csv files.\nThe script just looks to see if what is stored in the state code field of a csv file matches any code in the documentation. The results from three files are below.\nDelim file tests ================ 1995.txt: 0 matches and 680662 mismatches 2000.txt: 0 matches and 691060 mismatches 2019HwyBridgesDelimitedAllStates.txt: 0 matches and 617085 mismatches No state codes in these three files matched the codes from the documentation. However NBI also provides undelimited files, which I believe was the original format as the documentation implies this encoding. Since there is no delimiter, state code is defined as the first three digits in a line.\nUndelim file tests ================== WI19.txt: 14249 matches and 0 mismatches WY19.txt: 0 matches and 3114 mismatches MT19.txt: 5278 matches and 0 mismatches NC19.txt: 18407 matches and 0 mismatches WV19.txt: 7291 matches and 0 mismatches ND19.txt: 4329 matches and 0 mismatches al95.txt: 16576 matches and 0 mismatches ak95.txt: 1451 matches and 0 mismatches Except for WY19, state codes match.\nThere is another later (but undated) piece of documentation titled Specification for the National Bridge Inventory Bridge Elements which includes a different encoding for state codes which is shown below.\nThese encodings seem to match better if you do not consider leading zeros, but they will not work for undelimited files.\nBased on these antidotal results, it looks like there could have been an undocumented change in encoding schemes when files where made available in csv format, or a bug in the code used to do the conversion.\nWho cares? Is this a problem? This is in all likelihood not a big deal, but I am a nerd and spent an hour or two looking into it. This is likely just a case of a lack of documentation of small change in encoding practices. However, this data is used by researchers and regulators to understand the state of America\u0026rsquo;s infrastructure and therefore its representation should be as accurate and true to reality as possible. So at the very least I think it iw worth thinking about.\nUpdate I got a very timely response from Samantha Lubkin at the FHWA relating to the state encodings.\nThe State Code as defined in the NBI Coding Guide includes a 3rd digit that is currently obsolete. That digit is omitted from the delimited file, and can be ignored in the nondelimited file. Aside from that 3rd digit, the codes are identical between the SNBIBE and the NBI Coding Guide. (And both are based on FIPS.) Whether this kind of logic exists for other fields, I canâ€™t say. But yes, the NBI Coding Guide is the appropriate reference for both delimited and nondelimited NBI files. Looks like it was a one-ff thing with just state codes. Thanks again Samantha!\n","date":"2021-02-09","permalink":"https://ethanholleman.com/posts/nbi_encoding/","tags":["programming","blogs","Python","National Bridge Inventory"],"title":"Potential NBI encoding error"},{"content":"This recipe (aside from the electronics) is derived from Joshua Weissman\u0026rsquo;s video, How to Make Real Tonkotsu Ramen.\nIngredients Below are all materials you will need to prepare the soup.\nBroth / soup Ingredient Quantity Units Pig trotters 3 lbs Green onion 1 bunch Yellow onion 1 Shallot 2 Knob ginger 2 inches Ramen noodles 1 package Chashu (Braised pork belly) Ingredient Quantity Units Pork belly 2 lbs Soy sauce 1/2 cup Mirin 3/4 cup Sake 1 cup Ginger knob 2 inches Green onion 1 bunch Glove garlic 5 cloves Tare Ingredient Quantity Units Bonito flakes 1/2 cup Kombu 3 pieces Soy sauce 3/4 cup Dried shiitake mushrooms 1/4 cup Chashu braising liquid 1/4 cup Electronics (Optional) Ingredient Quantity Raspberry Pi 1 DS18B20 Temperature Sensor Module Kit 1 Protocol Prepare electronics So I was planning on monitoring the soup using the DS18B20 temperature sensor but it did not arrive in time so unfortunately I don\u0026rsquo;t have data from that. I also thought it might be cool to monitor if the water level drops below a certain level by taking advantage of the conductivity of the soup by placing two separated wires into the soup at the minimum desired level. If a current can be measured between them (broth is acting as the switch) you don\u0026rsquo;t need to replace the water. I had a difficult time positioning the wires consistently given the boiling liquid and required occasional mixing.\nI believe this is is very possible but will require some more specific hardware and / or modification to the pot itself. Anyway, if full control over your soup is what you desire you can download the code I wrote in anticipation of doing this at this link.\nOnce everything is set up, it should text / email you if soup temperature or broth level gets too low.\nPrepare broth Add 3lbs trotters to large pot. Cover with 2 inches of water and bring to a rolling boil for 10 minutes. Remove any residue that floats to the top with a spoon or mesh strainer. Empty 3lbs trotters and water into a strainer and rinse to clean. Place trotters back into the large pot and cover with 3 inches of water. Cut 1 bunch green onion into fourths and add to large pot. Cut 2 shallots into fourths, leaving skins on, and add to pot. Cut 1 yellow onion into fourths and add to the pot. Peel 2 2 inch knobs ginger, slice and add to pot. Peel 5 cloves garlic and add to pot. Bring water in large part to a rolling boil. Minimize temperature while still maintaining a low boil. This will be maintained for 12 hours. Stir pot around once an hour. Additionally, add water to restore original level whenever half of the water had been lost. Prepare chashu Cut 1 bunch green onion into 2 inch sections. Peel 2 inch knob ginger and slice. Add 1 cup sake, the green onion, the ginger, 3/4 cup mirin, 1/2 cup + 2 Tablespoons soy sauce, four cloves of whole garlic, and 1/3 cup water to a large oven-safe pot. Roll the pork belly length wise and tie with kitchen twine into three separate segments. Place into the pot with other ingredients. Bring chashu to a boil, then low boil and then cover with a lid and place into a 300 degree oven for 3-3.5 hours. Baste every 30 mins. Prepare the tare Add three 2 inch peices of kombu to a small pot. Add 3/4 cup water to the pot. Boil the water and then let kombu seep for 10 minutes. Add 1/2 cup bonito flakes to pot and let steep for 5 minutes. After the 5 minutes poor solution through a fine mesh sieve into a medium bowl. Add 3/4 cup soy sauce, 1/4 cup + 2 tablespoons mirin, 1/4 cup chashu braising liquid and 1/4 cup rehydrating liquid used for mushrooms to medium bowl. Stir and lightly salt. Prepare toppings This should be done within a half-hour of time to serve.\nFinely chop the green part of a green onion. Dehydrate shiitake mushrooms according to package directions. Prepare ice water bath. Bring small pot of water to boil. Add eggs and reduce heat to gentile bowl. Boil eggs for 6-7 minutes then place into ice bath. Assembling the soup Boil ramen noodles according to package directions. Slice chashu into into 1/2 inch slices. If cold reheat in pan. Add tare to bowl to taste. Strain broth and add to bowl. Add soft boiled eggs, chashu, mushrooms and sliced green onion to broth. Done!\nEnjoy Here are a few pictures of my results.\nChashu just out of the oven\nCloseup glam shot of that molten egg and soup\nAbusing portrait mode\n","date":"2021-01-14","permalink":"https://ethanholleman.com/posts/tonkatsu/","tags":["cooking","blogs"],"title":"Tonkotsu Recipe"},{"content":"The past couple days I have been running some ligand docking simulations as part of my current rotation with the Cortopasssi lab using Rosetta. One of these docking simulations involved fitting a small portion of the insulin receptor (IR) the lab is interested in, into a known binding region of the Shc1 protein.\nAny Rosetta docking simulation will require hundreds of repetitions, which generate a significant number of pdb files which show the final conformation of the protein and ligand at the end of a given simulation.\nWhile reading about the best way to aggregate and do analyise on these results I spent a bit of time looking for ways to visualize everything Rosetta spits out.\nThis was partly to sanity check the results quickly and also because 3D protein structures and plots just tend to look cool.\nIndividual images using PyMOL The initial simulation I ran produced 200 pdb files. One approach would be to create images of the ligand-protein interface for each of these pdb files.\nTo do this for 200 images I created a very hacky Python script that collects all the pdb files in a directory then creates a temporary PyMOL script which takes a nice picture of the ligand-protein interface.\nThis Python script is basically just a for loop but below is the PyMOL script that I used. All the {} are filled in using the format function in python with the correct filepaths for a specific pdb file.\nload {} # load this pdb file hide everything set cartoon_fancy_helices = 1 set cartoon_highlight_color = grey70 bg_colour white set antialias = 1 set ortho = 1 set sphere_mode, 5 select ligand, hetatm select pocket, ligand around 4 select pocket, byres pocket distance contacts, ligand, pocket, 4, 2 color 4, contacts preset.ligand_cartoon('all') cmd.show('cartoon', 'all') cmd.delete('docked_pol_conts') cmd.show('cartoon', 'all') cmd.hide('lines') cmd.bg_color('black') cmd.set('cartoon_fancy_helices', 1) cmd.show('sticks', 'pocket \u0026amp; (!(n;c,o,h|(n. n\u0026amp;!r. pro)))') cmd.show('sticks', 'ligand') cmd.hide('lines', 'hydro') cmd.hide('sticks', 'hydro') cmd.center('ligand') cmd.center('ligand') ray 1000,1500 png {} # save the image to this file quit This produces image like the one below.\nI then used the pillow library along with the score.sc file produced by Rosetta which has metrics on the docking quality to create a (huge) gif of all these images.\nFor some reason I was having an issue where if I changed the font of the text to anything other than the default font, when converted to a gif the text would not render. If anyone has had a similar problem please reach out.\nVisualizing the ensemble using plotly and R The second approach I used was to extract the coordinates of the protein and all ligand conformations from the pdb files and plot them in three dimensions using plotly. This produces a \u0026ldquo;cloud\u0026rdquo; of ligand conformations and is a more easily accessible sanity check to make sure Rosetta was docking in generally reasonable locations based on the input parameters.\nThe atoms are labeled with their identity (either protein or ligand) and their coordinates. They are colored by the Rosetta total_score parameter for the complex. Therefore if the complex scored -120, all atoms of the ligand in that complex will have the color corresponding to the -120 value.\nYou can pan, zoom, and rotate the plot using the controls in the upper right. The top 100 ligand conformations by total score are shown below.\nAnd the top 100 ligand conformations by interface delta X (difference in stability between bound and unbound states) are below this text.\n","date":"2021-01-12","permalink":"https://ethanholleman.com/posts/ligand_plotting/","tags":["programming","blogs","Python","R","PyMOL"],"title":"Visualizing ligand docking results with PyMOL scripting and R"},{"content":"I spent most of the day today learning about Javascript and CSS by building a (very ameutur) website that you can use to test your Poker pot odds calculation skills.\nDetermining pot odds is useful as when compared to the probability of winning a hand the call\u0026rsquo;s expected value can be approximated.\nYou can visit the website at potoddsquiz.com view the the code at the GitHub page or use the website embedded directly below.\nThis is the first Javascript project I have built from scratch and hosted somewhere and was a great way to start learning more about the very basics of web development and the Javascript language.\n","date":"2021-01-02","permalink":"https://ethanholleman.com/posts/potodds/","tags":["programming","blogs","poker"],"title":"PotOddsQuiz.com: A 1 day intro to Javascript and CSS"},{"content":"Are you using the UC Davis FARM for molecular modeling and need to figure out how to setup GROMACS? Well hello extremely small subset of the population! This is the guide for you.\nNote, this is only for a basic installation. For maximum performance refer to the GROMACS guide linked above.\nGetting started We will be working off the installation instructions on the GROMACS website but will modify a few steps to deal with the quirks of the FARM at the time of writing and the fact you will not have sudo privileges.\nIf you want to cut to the chase, you can run this script, which will run all the code in this guide in one go. Everything will be downloaded / complied in the directory you run the script in. You can use the command below to download the script, give it permission to run, and then run it.\nwget http://ethanholleman.github.io/posts/code/farm_install_gromacs.sh; chmod 777 farm_install_gromacs.sh; ./farm_install_gromacs.sh With that in mind the first thing to do is select a directory where you will download everything, cd into it and get going.\nDownload and install a recent cmake version At the time of writing, the FARM is running cmake 3.10.2 while GROMACS requires at least 3.13.0 to build. I will be downloading 3.19.2 as it is the most recent at the time of writing. If you are downloading a different version you will have to modify some of the commands to reflect that (this will be true for all downloaded programs).\nDownload and run the install with the commands below\nwget https://github.com/Kitware/CMake/releases/download/v3.19.2/cmake-3.19.2-Linux-x86_64.sh chmod 777 cmake-3.19.2-Linux-x86_64.sh echo \u0026quot;Running cmake installer\u0026quot; ./cmake-3.19.2-Linux-x86_64.sh Download and compile GROMACS Download your preferred version of GROMACS from the docs, I will be using 2021-rc1 for this guide.\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2021-rc1.tar.gz tar xfz gromacs-2021-rc1.tar.gz Now enter into the newly downloaded GROMACS directory and create a build directory.\ncd gromacs-2021-rc1 mkdir build cd build From the build directory run the newly downloaded cmake version by determinging the path to the cmake exe. It will be located in folder produced by the cmake installer. For me, the full path was /home/ethollem/software/cmake-3.19.2-Linux-x86_64/cmake-3.19.2-Linux-x86_64/bin/cmake.\nThen run cmake with your version subsituted into [cmake] below.\n[cmake] .. -DGMX_BUILD_OWN_FFTW=ON -DREGRESSIONTEST_DOWNLOAD=ON Then make the program (this could take a while).\nmake make check The GROMACS guide recommends the following command\nsudo make install but since we do not have sudo privileges to run the program we can add the exe to out PATH variable. From the build directory the GROMACS exe path will be [Your absolute path to build dir]/bin/gmx. For me this looks like home/ethollem/software/gromacs-2021-rc1/build/bin/gmx.\nOpen / create ~/.bashrc using your preferred text editor and add the line\nexport PATH=\u0026quot;$HOME/software/gromacs-2021-rc1/build/bin/gmx:$PATH\u0026quot; Then log off and log back in again. Test everything is working correctly by running\ngmx --help You should be greeted wih the GROMACS help page. If you are you are now good to go.\n","date":"2020-12-30","permalink":"https://ethanholleman.com/posts/gromacs_install/","tags":["programming","blogs","guides"],"title":"Installing GROMACS on the UC Davis FARM cluster: or install GROMACS without sudo privileges."},{"content":"Day trip to Yosemite National Park.\nHike up to mirror lake Campground near Curry Village\nGood advice.\nEthan with log on the trail.\nErica with boulder.\nMoss 1.\nMoss 2.\nMoss 3 (lots of interesting mosses).\nAt mirror lake Mirror Lake visitor info placard.\nMirror lake facing Northwest towards Mt. Watkins.\nHalf Dome.\nEthan and Erica in front of Half Dome.\nInteresting log, ground in the log\u0026rsquo;s shadow remained frozen.\nPanoramic view facing Northeast with (from left to right) Mt. Watkins, Ahwiyah Point and Half Dome in view.\n","date":"2020-12-29","permalink":"https://ethanholleman.com/posts/yosemite/","tags":["day trips","national parks","blogs"],"title":"Day trip to Yosemite"},{"content":"Skribbl.io is a great free quarantine / social distanced game where one person attempts to draw a word while everyone else guesses what they are drawing. When setting up the game you can supply your own list of comma separated words doing the game.\nThe problem with doing this manually is that one person playing will know all the words.\nFor an upcoming Zoom party I created a python command line application that takes in subreddit names and a few other parameters and using the Praw library retrieves the most commonly used words from the top comments of posts to a subreddit.\nSince subreddits are generally devoted to a specific topic you can easily create pseudo-themed word banks by pulling comments from a category of topics and selecting subreddits under that banner.\nYou can download the program from the GitHub page.\nUsage Install dependencies The only dependcy you need is Praw. Install it with the command below.\npip install praw Setup API keys If you want to run the program yourself you will need to get a client id and client_secret to use the Reddit API through Praw. The tutorial below has all the info you need (you only need to watch the setup portion).\nTo be able to post this project on GitHub (relatively) safely I used environmental variables to store the values or my Reddit API credentials. You can do the same or modify the code of collect_reddit_instance function (shown below) in collect.py to use your credentials.\ndef create_reddit_instance(): '''Create a Reddit instance using Praw library. Returns: Reddit: Reddit instance via Praw. Credentials set using environmental variables. ''' return praw.Reddit(client_id=os.environ['PRAW_CLIENT_ID'], client_secret=os.environ['PRAW_SECRET'], user_agent=os.environ['PRAW_USER_AGENT'] ) Set values for PRAW_CLIENT_ID, PRAW_SECRET and PRAW_USER_AGENT or modify the code directly with your credentials.\nRun the program Once you have that set up you are ready to run the program by executing the run.py file. The help menu it will print is below.\npython run.py --help usage: run.py [-h] [-r SUBREDDITS [SUBREDDITS ...]] [-n NUMBER_WORDS] [-mc COMMENT_LIMIT] [-o OUTPUT_DIR] [-f] [-p POST_LIMIT] [-l MIN_WORD_LENGTH] Harvest frequently words from subreddit comments using Praw optional arguments: -h, --help show this help message and exit -r SUBREDDITS [SUBREDDITS ...], --subreddits SUBREDDITS [SUBREDDITS ...] List of subreddits to pull comments from -n NUMBER_WORDS, --number_words NUMBER_WORDS Number of words to return per subreddit. Defaults to 25. -mc COMMENT_LIMIT, --comment_limit COMMENT_LIMIT Max number of comments to harvest per subreddit. Defaults to 10,000 -o OUTPUT_DIR, --output_dir OUTPUT_DIR Output directory to write results to. -f, --include_occurrences Include the number of times each word appears in the final output -p POST_LIMIT, --post_limit POST_LIMIT Max number of posts to harvest from. Defaults to 50. -l MIN_WORD_LENGTH, --min_word_length MIN_WORD_LENGTH Min word length. Defaults to 3 characters. For example if I wanted to get the 10 most frequently used words from 100 comments from r/DataHoarder I would use the command\npython run.py -r \u0026quot;DataHoarder\u0026quot; -n 10 -mc 100 You can also specify multiple subreddits. The top word for each subreddit will be written to a separate text files.\npython run.py -r \u0026quot;DataHoarder\u0026quot; \u0026quot;Python\u0026quot; \u0026quot;arduino\u0026quot; -n 10 -mc 100 Or use pre-harvested words If you do not want to set up the program on your own computer I have already created lists of 25 most used words with 5 or more characters from top comments of the 100 most popular subreddits.\nYou can download those files from the GitHib page here. Words are listed on a single line separated by command for easier input into\nNOTE\nI have not reviewed all the words in these files and do not endorse any of the content that may be found within, this is the internet after all.\n","date":"2020-12-28","permalink":"https://ethanholleman.com/posts/skribbl.io_words/","tags":["blogs","Python"],"title":"Make custom Skribbl.io word banks using Reddit and Praw"},{"content":"After finding the COG-UK data I was looking around for other interesting COVID-19 datasets to play around with and build my R plotting skills with.\nUser moritz.kraemer posted this article on early case descriptions which included a lot of geo-spacial data that I was interested in takeing a look at.\nThere was a significant number of fields devoted to hospitalization related measurements and so I focused on that subject for the plot below.\nThe dataset includes patients with and without hospitalization records and so first I filtered down to just those with records and those who also had location data. This subset of patients formed subplot A. Clearly this is not an exhaustive dataset and seems to focus on hospitalizations for China, France, and Argentina.\nSubplot B shows hospitalizations by date, which for this dataset, really pick up in April. However, this is not super informative due to ~90 % of the data being localized to three countries.\nFinally, subplot C includes two boxplots. The first shows the difference in days between a patient experiencing symptoms and admission to a hospital. A negative value indicates days before admission. So a patient with a value of -10 would mean the patient experienced symptoms 10 days before being admitted. The second boxplot shows the length of stay once admitted. It should be noted that this dataset lumps deaths and discharges into the same category date_death_or_discharge. The mean stay once admitted was ~10 days.\nThe R code to generate these plots can be viewed here\n","date":"2020-12-24","permalink":"https://ethanholleman.com/posts/covid_data/","tags":["programming","R","blogs"],"title":"Plotting COVID-19 Hospitalization Geo-Spacial Data"},{"content":"The Covid-19 Genomics UK Consortium has been collecting and sequencing thousands of COVID-19 genomes from patients in the UK and around the world.\nAll of their data is publicly available. Here I played around with the phylogenetic tree they have created from global alignments of all the genomes they have sequenced.\nYou can download the tree in Newick format from their data page which also hosts sequences and the alignment files.\nVisualizing the COVID-19 phylogenetic tree by country of origin Genome count by country Note this plot is log scale in the y-axis.\n16 most prevalent UK COVID-19 lineages Density plots showing the number of genomes of the 16 most prevalent lineages detected by COG-UK.\nCov-lineages has a lot of good information on all of these lineages, including B.1.177 which has recently set off alarms as the new \u0026ldquo;mutant UK strain\u0026rdquo;.\nCode used to make the above plots can be viewed here.\n","date":"2020-12-21","permalink":"https://ethanholleman.com/posts/cog_sars/","tags":["programming","R","blogs"],"title":"Plotting COG-UK Data"},{"content":"Apartment Tour Taken right after move in.\nSkys during the wildfires Those are not clouds.\nDavis campus cows Around Davis Hiking in Winters, CA\nFishing at Putah Creek\nThanksgiving Bubbles Lots cat pictures have been taken\nCalifornia changes a man ","date":"2020-12-20","permalink":"https://ethanholleman.com/posts/christmas_card/","tags":["blogs","christmas card"],"title":"2020 Christmas Card Bonus Pictures"},{"content":"Y axis of all plots are unit-less.\nRunning stack sizes Shows each players cumulative stack over all nights played.\nWinnings by date Plot of each players winnings (stack at the end of the round - buy in) over time.\nAll games have been played virtually, for info on how we host these see this post.\n","date":"2020-12-18","permalink":"https://ethanholleman.com/posts/running_winnings/","tags":["poker","blogs"],"title":"Poker Nights Running Results"},{"content":"Update I recently found a number of .io type websites devoted specifically to poker (not sure why didn\u0026rsquo;t think to search poker .io earlier) which get around the main issues of playing using playingcards.io.\nI would highly, highly recommend using lipoker.io. It handles betting with shortcuts for bets based on pot or blind sizes, handles turns automatically and does not require a sign up.\nFor Poker, lipoker is supporior in every way to PlayingCards.io due to being designed for this specific game. If for some reason you would still like to use PlayingCards.io continue reading, otherwise stop and use lipoker.io.\nRecently a few of my friends and I wanted to do virtual Poker night. We where not interested in playing for cash and so I started looking around for an online platform to make it happen.\nPretty much everything that comes up with a cursory google search did not satisfy my basic requirements but eventually I came across PlayingCards.io which provided what I think is the best solution for free, causal quarantine poker nights.\nStep 1 is to set up your \u0026ldquo;table\u0026rdquo; with PlayingCards. Use this link to start building a table. Later once you finish setting everything up you can share the link to the table and others will be able to join, see and manipulate the objects on the table.\nYou can use the automation buttons to handle things like dealing, the flop, turn, river shuffling, as well as betting. The room I set up looks like this.\nYou can deal cards to each player using the Deal button and show each round of community cards using the respectively labeled button. Empty sets the pot to 0, Reset sets the pot to 0 and all player stacks to 100 and Reveal shows all players cards.\nThe only thing that is a little clunky is betting and collecting your winnings. The automations allow you to add or subtract a value from a counter (which are used to keep track of each player\u0026rsquo;s stack and the current pot) but not read from the state of another counter. This means you cannot create a button that adds the current value in the pot to a player\u0026rsquo;s stack - this needs to be done by hand.\nAlso anyone can press any button at any time; just something to keep in mind if playing with anyone that you thought of when reading that sentence.\nNote that cards placed into the \u0026ldquo;hand\u0026rdquo; area can only be seen by the player that put them there. This makes keeping your pocket cards secret from everyone else possible.\nIf you don\u0026rsquo;t want to set up your own room from scratch you can download the room I set up and then load from that file.\nNo Poker is actually done over Zoom, it is only used as the video call medium.\nIt definitely look a couple hands to get used to using the interface but once things got going it worked really well. We found it also helps to keep track of your stack either on paper or using small text boxes on the virtual table.\nHere is a gif of me playing out a hand. Normally the other players would take their cards out of the dealt slots and keep them in their \u0026ldquo;hand\u0026rdquo; area where they are not visible to other players until showdown. If a player decides to fold they place their cards back into their dealt slots.\n","date":"2020-12-15","permalink":"https://ethanholleman.com/posts/zoom_poker/","tags":["poker","blogs"],"title":"Casual virtual Poker with Zoom and PlayingCards.io"},{"content":"Turning the jungle into punch cards During the early optimistic days of the summer 2020 quarantine I watched Ken Burn\u0026rsquo;s fantastic 10 part 18-hour series on the Vietnam war. It is by far the most accessible and compressive body of work on the subject. Burn\u0026rsquo;s starts you off pre-WWI so you really get a comprehensive picture of things.\nOne of the aspects of the war that fascinated me the most was the push by the the then Secretary of Defense, Robert McNamara, to quantify as much of the war as was computationally possible. One of the most storage intensive of McNamara\u0026rsquo;s efforts was the Hamlet Evaluation System, which attempted to quantify the degree to which ~12,000 small, rural Vietnamese villages had been pacified. A RAND corporation report found the program was producing 90,000 pages of data every month. More than could have ever been useful.\nExample of data produced by the Hamlet Evaluation Program showing change in hamlet classifications over time This was the data produced by just one wartime metrics program. Even if the DoD had the raw 60s era computing power and the army of FORTRAN programers that would have been needed to wrangle it all, the metrics themselves were questionable at best. One of the historians interviewed as apart of Burn\u0026rsquo;s series said something along the lines of \u0026ldquo;When you can\u0026rsquo;t measure whats counts, you make what can count the measure\u0026rdquo;.\nI was interested in actually seeing what that data looked like in its raw form. What where programmers of the era actually looking at and wrangling when some Army big-wig said \u0026ldquo;We need to be producing 90,000 pages of data a month\u0026rdquo;? So I did some googling and dug into a couple documents hosted my the National Archives to try and get a picture of what Vietnam looked like from the perspective of a punch card.\nThe Phung Hoang Mangement Information System The degree of documentation relating to the Hamlet Evaluation System that survives to today is, somewhat unsurprisingly, extremely large. So picking one out to dive into was a less than analytical process.\nA came across the Phung Hoang Mangement Information System, PHMIS, (MACV Document Number DAR R33 CM-01A, March 1972) which was later replaced with the National Police Infrastructure Analysis Subsystem; a database cataloging the personal information of Vietnamese who where suspected of or convicted of aiding communist forces as part of the Hamlet Evaluation Program. This entry was interesting to me because it had both the technical documentation needed to actually make sense of the data and because of its historical context. The PHMIS was used as a catalogue for the operations of the Phoenix program a CIA headed counter-insurgency operation that among other techniques, employed torture and assassination to identify and kill Viet-Cong and Viet-Cong collaborators.\nFlowchart depicting data schema of the Phung Hoang Mangement Information System, March 1972 report The Phoenix Program was, deservingly, a locus of controversy within a storm of controversies surrounding US operations in Vietnam, eventually building to a series of congressional hearings in 1971. The data stored in this National Archive entry, in some way, is the bureaucratic reflection of all the individual stories and lives impacted by this corner of red-scare induced mania.\nGetting into the data The PHMIS National Archive entry contains one main data file and some aggregated technical documentation.\nRG330.PHMIS.P6972: The actual binary data file. Refered to as RG330 222.1SPA.pdf: Historical background on the PHMIS 222.1DP.pdf: Technical documentation on format of RG330 At first I was a bit lost as to where to start after thumbing through the technical documentation. At first I naively tried to read RG330 as UTF-8 but then soon remembered unicode was not created until the 1980s.\nThankfully I found the technical specifications summary provided by the National Archives which had this critical information.\nThe number of records, the length of each record and critically the encoding: EBCDIC. At this point I had no idea what EBCDIC encoding was but some quick googling corrected that. Basically, EBCDIC (Extended Binary Coded Decimal Interchange Code) was created by IBM in the late 1950s and used a perplexing (see EBCDIC wikipedia page criticism and humor) eight-bit character encoding for mainframe computers.\nExample of EBCDIC encoded punch card. I almost stopped here, but the Python community came through once again and thanks to Thomas Aglassinger you can successfully run the command\npip install ebcdic and have a number of EBCDIC encodings to work with right in Python. And the secrets of RG330 can be revealed in their UTF-8 glory with the Python snippet below.\nimport ebcdic filepath = './RG330.PHMIS.P6972' data = open(filepath, 'rb').read().decode('cp1141') Wrangling The next step was to get the decoded mass of information into something that could be output to a csv file and would be much easier to work with. The technical documentation that let us know RG330 was EDCDIC encoded also says that the record length is 319 so my approach was to go with that and pray to the data-wrangling Gods.\nstep = 319 print(data[:step]) gives\n01000005A207000069120H691270122E70125041100002CB 34KKKKK5C12070103001BM########################00000Ã¤ 00000Ã¤ 00000Ã¤ 00000Ã¤ 00000Ã¤ 00000Ã¤ 00000Ã¤ 00000Ã¤ 0Ã¤ ######################################################################## which as a first attempt does not actually look that bad. The National Archives documentation notes that a personally identifying information in the database has been redacted for public use - which is showing up as the # characters. A memo from December 10, 1992 states the following.\nI used this to verify the correctness of my naive parsing approach. If everything lines up, the only characters we should be finding in columns 73-96, 248-271, 272-295 and 296-319 are #s.\nredacted_base_1 = [(73, 96), (248, 271), (272, 295), (296, 319)] redacted_base_0 = [(r[0] - 1, r[1]) for r in redacted_base_1] for redacted_region in redacted_base_0: start, end = redacted_region assert set(data[start:end]).pop() == \u0026quot;#\u0026quot; Does not produce any assertion errors so lets expand to the whole dataset.\nfor i in range(0, len(data), 319): record = data[i:i+step] for redacted_region in redacted_base_0: start, end = redacted_region assert set(record[start:end]).pop() == \u0026quot;#\u0026quot; Which also does not produce any assertion errors, so things are looking pretty good at this point. The two items of main concern are the Ã¤ characters and the large amount of whitespace. Some more investigation revealed the reason for the whitespace. The document was originally stored in a variable record length format and was converted to a fixed length by the National Archives at some point. Relevant documentation in its original photocopy glory below.\nWhile I am not as certain about the cause of the diacritic a character, I believe it is due to the use of zoned decimal formatting in some of the fields. The documentation notes its use in general, but does not provide specific indices. Since it is not affecting the integrity of the actual record parsing I am ignoring apparent zoned decimal fields for the time being.\nThe technical documentation also defines the range of each field within a record that we can use to make the final output file more explicitly delimitated. A sample of which is shown below.\nSo now it is just a matter of creating a function that will cut up a record based on the fields defined in this table. I first used PyPDF2 to pull out the text from the table I was interested in.\nimport PyPDF2 pdf = open('222.1DP.pdf', 'rb') reader = PyPDF2.PdfFileReader(pdf) pages = ''.join([reader.getPage(p).extractText() for p in [20, 21, 22, 23]]) print(pages[:100]) Which looks like this\nINPUT, OUTPUT, MASTER DEFINITION (Excluding Reports) I. PAGE 1 OF 4 5. DATE PREPARED 9/1/77 2. NAME Its a jumble but thankfully, there are some patterns that can be exploited to reduce manual work required to get everything correctly formatted. Specifically after the range of each field it is followed by either an A or a N, signifying if that data is numeric or alphanumeric. We can use this pattern in a regex to roughly pull out what we need.\nimport re finder = re.compile(r'(.{1,7}) (\\d+-?\\d*) (N|A)') matches = finder.findall(pages) for m in matches[:10]: print(m[0], m[1]) Which prints\n. PROVe 1-2 SE'O'fO 3-8 6 ATLRG 9 !XX)RP 10 DPROV 11-12 . DDIsr 13-14 2 DVllL 15-16 . IDATX 23-26 BDA'IX 33-36 POORP 37 While this is by no means perfect it is looking a lot better. From here I just cleaned things up manually, eventually creating a text file that looks like this\nPROVC 1 2 SEQNO 3 8 ATLRG 9 DOORP 10 . . . ALTVCI 239 247 MOM 248 271 POP 272 295 ALIAS 296 319 Now we can finally create the function that will create the csv file we are after!\nFirst I created a dictionary that could be used to slice each \u0026ldquo;row\u0026rdquo; of the raw data.\nimport csv table = \u0026quot;parse_table.txt\u0026quot; spacing_dict = { r[0]: [int(i) for i in r[1:]] for r in csv.reader(open(table), delimiter=' ') } # adjust for base 1 to base 0 indexing for field in spacing_dict: spacing_dict[field][0] -= 1 # append next index for single index fields for easier slicing in next step for field_name in spacing_dict: if len(spacing_dict[field_name]) == 1: val = spacing_dict[field_name][0] spacing_dict[field_name].append(val+1) Then I created a function that would actually do the slicing on each raw data row and return a dictionary with the field names as keys and the data in each field\u0026rsquo;s respective domain as values.\ndef raw_data_to_field_dict(raw_data, spacing_dict): field_dict = {} for field_name in spacing_dict: start, end = spacing_dict[field_name] field_dict[field_name] = raw_data[start:end] return field_dict Now we can test this out to see if we can write our csv file.\ndef write_csv_from_raw_data(raw_data, spacing_dict, outname='data.csv'): csv_rows = [] for i in range(0, len(data), step): row = data[i:i+step] csv_rows.append(raw_data_to_field_dict(row, spacing_dict)) with open(outname, 'w') as handle: writer = csv.DictWriter(handle, fieldnames=spacing_dict.keys()) writer.writeheader() writer.writerows(csv_rows) The function executes without incident and writes a csv file that looks like the sample below.\nPROVC\tSEQNO\tATLRG\tDOORP\tDPROV\tDDIST\tDVllL 1\t5\t7\t0\t0 1\t11\t1\t2\t0 1\t12\t1\t2\t0 Much easier to read!\nVisualizing Now that the data is in a more accessible format we can start to take examine what it looks like using R and the ggplot2 package.\nArrests by year Unsurprisingly most arrests took place between 70 and 72. Although there were some extreme early outliers in 1900 and 1903 which are likely data entry errors.\nIndividual statuses Fields 126-131 contain the OPINFO information, which the technical documentation describes as a group containing the following information, again as described by the technical documentation.\nSTATUS: The status of an individual. TARGET: The type of target, general of specific. LEVEL: The level of operation. Corps, Division, etc. FORCE: Type of action force which was responsible for the neutralization. DEFAC: Place of detention. This is a lot of interesting information. These values are all stored as one letter codes and the documentation provides tables for translating them, like the one below which is used to translate the STATUS code.\nFirst we can look at just statuses of all individuals in the database to get as sense for what was happening to the people targeted by Project Phoenix.\nWhile the STATUS field is missing for about a third of individuals in the database, clearly the most common outcomes were \u0026ldquo;captured\u0026rdquo;, \u0026ldquo;killed\u0026rdquo; or \u0026ldquo;rallied\u0026rdquo;. While \u0026ldquo;captured\u0026rdquo; and \u0026ldquo;killed\u0026rdquo; are relatively unambiguous, there is not further explanation of what \u0026ldquo;rallied\u0026rdquo; refers to that I could find.\nThis same data can also be broken down in a few interesting ways. We can plot the same STATUS data but split into into subplots based on the FORCE that was responsible for the (coldly bureaucratically termed) neutralization.\n\\\nUsing this plot, we can see that \u0026ldquo;Regional Forces\u0026rdquo; where the most active group, with \u0026ldquo;ARVN Main Forces\u0026rdquo;, \u0026ldquo;Popular Forces\u0026rdquo;, \u0026ldquo;Provincial Reconnaissance Unit\u0026rdquo; making up much of the remainder.\nThis plot also shows that US Forces, at least according to this database, where not as nearly as directly involved as organizations that can be grouped into the \u0026ldquo;South Vietnamese Allies\u0026rdquo; category.\nLastly, I was interested in what this program looked like over time. Individuals that were captured (opposed to killed outright) usually had a value in their SADATX field: \u0026ldquo;Date of sentence action in YYMM order\u0026rdquo;. I used this as a proxy for a given group\u0026rsquo;s activity over time, granted this would be easily skewed in the case one group was tasked explicitly with capturing while another was tasked with killing. Plotting SADATX vs the number of individuals for all groups listed by the FORCES field produced the plot below.\nThere is still much to be said I only looked at a small part of this dataset, but there is still much more to be gleaned. If you would like to play around with the data yourself you can download the csv file I produced from this link. You can also download all the documentation I referenced from the National Archives entry at this link.\nThank you for reading.\n-eth\n","date":"2020-12-12","permalink":"https://ethanholleman.com/posts/vietnam_data/","tags":["blogs","data wrangling"],"title":"Data and the Vietnam War"},{"content":"Day trip out to Putah Creek in Winters CA.\nA few turkeys we saw before leaving Davis.\nThe spot in the creek we picked out.\nErica practicing casting.\nTesting out the waders.\n","date":"2020-11-28","permalink":"https://ethanholleman.com/posts/putah_fishing/","tags":["fishing","blogs"],"title":"Fishing at Putah Creek"},{"content":"I ran into a few issues trying to figure out how to run batch scripts on the Genome Center cluster. One of the least documented and hardest to figure out was how to successfully load R packages I had installed to my user directory.\nFor example I would start R and install a package I needed and load the library with no problem.\nThe code below would run without a problem.\ninstall.packages(\u0026quot;glmnet\u0026quot;) library(glmnet) But then calling the running the same script from a batch file fails to import the library.\nI tried multiple possible fixes with no success until I was tipped of by a current lab member about the magic word.\naklog For some reason putting this right after the SBATCH lines in the script changed something that allowed SLURM to see the R packages in my user directory.\nIt is not currently documented, but when it hopefully is I will link to the page here.\n","date":"2020-11-03","permalink":"https://ethanholleman.com/posts/genome_cluster/","tags":["programming","blogs","guides"],"title":"Using local R packages on UC Davis Genome Center cluster"},{"content":"This was a letter I wrote to my local high school district board during the early days of lock down titled \u0026ldquo;Creative Approaches in This Coming School Year\u0026rdquo;.\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":"2020-07-24","permalink":"https://ethanholleman.com/posts/letter_to_d113/","tags":["blogs","writing"],"title":"Letter to D113 school board"},{"content":"Should you buy a 3D printer in 2023? There are an increasing number of short form videos floating around showing people creating various objects with their 3D printers. These are great at increasing the visibility of this frankly incredible technology but as with most infomration in this format. It fails to convey the whole story behind 3D printer ownership.\nSo I wanted to put this post out there as someone who has (according to my printer statistics) printed over 7.5 kilometers worth of filament in the past year and a half or so before you purchase for yourself, or someone else a new 3D printer.\nThe key consideration, in my opinion, is this.\nAt this point in time, 3D printers still run best when the operator knows exactly how they work. This is unlike other complex pieces of technology like cars, microwaves, personal computers etc. which are largely \u0026ldquo;black-box-able\u0026rdquo; meaning that their successful and safe operation is largely not dependent on the user\u0026rsquo;s knowledge of the underlying systems of the device.\n","date":"0001-01-01","permalink":"https://ethanholleman.com/posts/should-i-buy-3d-printer/","tags":null,"title":""}]